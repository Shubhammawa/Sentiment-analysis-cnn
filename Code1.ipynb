{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import math\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from tensorflow.python.framework import ops\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv(\"../Sentiment-analysis-cnn/sentiment labelled sentences\",dtype=object,na_values=str).values\n",
    "#df = pd.read_fwf('../Sentiment-analysis-cnn/sentiment labelled sentences/imdb_labelled.txt')\n",
    "#df.to_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Sentiment-analysis-cnn/sentiment labelled sentences/imdb_labelled.txt', 'r') as in_file:\n",
    "    stripped = (line.strip() for line in in_file)\n",
    "    lines = (line.split('\\t') for line in stripped if line)\n",
    "    with open('train.csv', 'w') as out_file:\n",
    "        writer = csv.writer(out_file)\n",
    "        writer.writerow(('Sentence', 'Category'))\n",
    "        writer.writerows(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe = pd.read_csv(\"../Sentiment-analysis-cnn/sentiment labelled sentences/imdb_labelled.txt\",delimiter=\"\\t\")\n",
    "# dataframe.to_csv(\"NewProcessedDoc.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../Sentiment-analysis-cnn/sentiment labelled sentences/imdb_labelled.txt', \"r\",newline='') as in_text:\n",
    "#     in_reader = csv.reader(in_text, dialect='excel-tab')\n",
    "#     with open('train2.csv', \"w\") as out_csv:\n",
    "#         out_writer = csv.writer(out_csv,dialect='excel')\n",
    "#         for row in in_reader:\n",
    "#             out_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string2 = ';) Recommend with confidence!\\t1'\n",
    "# string = 'how he became the emperor; nothing about where he spend 20 years\\t0'\n",
    "# print(string.strip())\n",
    "# print(string.strip(';'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(string.strip().split('\\t'))\n",
    "# print(string.strip(';').split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# emoji_pattern = re.compile(\"[\"\n",
    "#         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "#         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "#         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "#         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "#                            \"]+\", flags=re.UNICODE)\n",
    "# text = ';) Recommend with confidence!\\t1'\n",
    "# print(emoji_pattern.sub(r'',text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\",dtype=object,na_values=str).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['A very, very, very slow-moving, aimless movie about a distressed, drifting young man.  '\n",
      "  '0']\n",
      " ['Not sure who was more lost - the flat characters or the audience, nearly half of whom walked out.  '\n",
      "  '0']\n",
      " ['Attempting artiness with black & white and clever camera angles, the movie disappointed - became even more ridiculous - as the acting was poor and the plot and lines almost non-existent.  '\n",
      "  '0']\n",
      " ['Very little music or anything to speak of.  ' '0']\n",
      " ['The best scene in the movie was when Gerardo is trying to find a song that keeps running through his head.  '\n",
      "  '1']]\n"
     ]
    }
   ],
   "source": [
    "print(data[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(data[:,0])\n",
    "y = np.array((data[:,1]))\n",
    "y = np.array([int(num) for num in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'numpy.int64'>\n",
      "(1000,)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "print(type(x[0]))\n",
    "print(type(y[0]))\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A very, very, very slow-moving, aimless movie about a distressed, drifting young man.  '\n",
      " 'Not sure who was more lost - the flat characters or the audience, nearly half of whom walked out.  '\n",
      " 'Attempting artiness with black & white and clever camera angles, the movie disappointed - became even more ridiculous - as the acting was poor and the plot and lines almost non-existent.  '\n",
      " 'Very little music or anything to speak of.  ']\n"
     ]
    }
   ],
   "source": [
    "print(x[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of lists of words\n",
    "words = []\n",
    "sentences = []\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "i = 0\n",
    "for sent in x:\n",
    "    #words.append(sent.split())\n",
    "    #i = i + 1\n",
    "    #print(i)\n",
    "    for word in tokenizer.tokenize(sent):\n",
    "        words.append(word.lower())\n",
    "    sentences.append(words)\n",
    "    words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a', 'very', 'very', 'very', 'slow', 'moving', 'aimless', 'movie', 'about', 'a', 'distressed', 'drifting', 'young', 'man'], ['not', 'sure', 'who', 'was', 'more', 'lost', 'the', 'flat', 'characters', 'or', 'the', 'audience', 'nearly', 'half', 'of', 'whom', 'walked', 'out'], ['attempting', 'artiness', 'with', 'black', 'white', 'and', 'clever', 'camera', 'angles', 'the', 'movie', 'disappointed', 'became', 'even', 'more', 'ridiculous', 'as', 'the', 'acting', 'was', 'poor', 'and', 'the', 'plot', 'and', 'lines', 'almost', 'non', 'existent'], ['very', 'little', 'music', 'or', 'anything', 'to', 'speak', 'of']]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0:4])\n",
    "print(type(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = np.array(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214946, 295820)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_word2vec = Word2Vec(sentences, size=150, window=5, min_count=0,workers=10,sg=0)\n",
    "model_word2vec.train(sentences,total_examples=len(sentences),epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=3074, size=150, alpha=0.025)\n",
      "[ 0.10902458 -0.21766782  0.1857366  -0.42383268  0.2789201  -0.4271565\n",
      " -0.28179136  0.36202356 -0.54728866  0.198906    0.17508337  0.03796723\n",
      "  0.3015405   0.30907395  0.2390647   0.44528037  0.16510423  0.23404522\n",
      "  0.15464565 -0.17487589  0.3140624  -0.10139158 -0.2591735  -0.4136518\n",
      "  0.3091127  -0.8201067  -0.6215937  -0.10718023  0.3624079  -0.26274678\n",
      " -0.05679674 -0.19720316 -0.02506198 -0.16061829 -0.12149503 -0.29100668\n",
      "  0.24888855 -0.1630668   0.24276835  0.0137295   0.20929584  0.2335969\n",
      " -0.32349855 -0.17507416  0.1198227  -0.9621831  -0.73184     0.27813846\n",
      " -0.19443697  0.3630736   0.08988742 -0.08139827 -0.20259918  0.44881845\n",
      "  0.40841454 -0.09288769 -0.6969565   0.1451689  -0.01426006  0.47925788\n",
      "  0.2852463  -0.34805042 -0.0291347  -0.07932296  0.25889465  0.32362327\n",
      "  0.22643991  0.4332494   0.56700337 -0.07116244 -0.25801364  0.05867479\n",
      "  0.19087964 -0.570881    0.5390056   0.5656091   0.29902452 -0.06451844\n",
      " -0.49886742 -0.03902031  0.1099821  -0.17007574 -0.14974684  0.3743148\n",
      "  0.33599797  0.3687561   0.35927075  0.03580436  0.4293621  -0.36624756\n",
      "  0.32606274 -0.27602047  0.20314531  0.09754738 -0.02149142  0.3842784\n",
      "  1.1194566  -0.5677539  -0.6275516  -0.14416759  0.5240499  -0.5877747\n",
      " -0.25290102  0.33180475 -0.46627507  0.251125   -0.27560923  0.01005101\n",
      "  0.08204332  0.41903073 -0.5176378   0.31964117 -0.38095355  0.4274798\n",
      " -0.10598955 -0.13326818  0.4826708  -0.47539106 -0.763153    0.5638431\n",
      " -0.02501504 -0.24700709  0.50458944 -0.14702088  0.46723324 -0.5778065\n",
      " -0.49708417 -0.5299094   0.3486963   0.22794046  0.37302148  0.03263467\n",
      "  0.5070221  -0.02484448 -0.2062162  -0.01637267  0.586243   -0.2159198\n",
      " -0.12874    -0.14204462  0.56305814  0.56730336 -0.05926124 -0.34529722\n",
      " -0.08166146  0.06199558  0.14764494  0.16643612 -0.03790693  0.06653803]\n",
      "[('very', 0.9998859167098999), ('well', 0.99985271692276), ('from', 0.9998521208763123), ('also', 0.9998263716697693), ('wonderful', 0.9998184442520142), ('character', 0.9998167157173157), ('script', 0.9998111128807068), ('was', 0.999809980392456), ('story', 0.9998064041137695), ('music', 0.99980229139328)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "/usr/local/lib/python3.5/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(model_word2vec)\n",
    "print(model_word2vec['good'])\n",
    "print(model_word2vec.wv.most_similar(\"good\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word2vec.save(\"Saved_model_word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=3074, size=150, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load(\"Saved_model_word2vec\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len([2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "temp = []\n",
    "for i in range(0,len(sentences)):\n",
    "    for j in range(0,len(sentences[i])):\n",
    "        temp.append(model[sentences[i][j]])\n",
    "    X.append(temp)\n",
    "    temp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(len(X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n",
      "620\n"
     ]
    }
   ],
   "source": [
    "max1 = 0\n",
    "for i in range(0,len(X)):\n",
    "    if(len(X[i])>max1):\n",
    "        max1 = len(X[i])\n",
    "        pos = i\n",
    "print(max1)\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(0,len(X)):\n",
    "    if(len(X[i])>64):\n",
    "        count = count + 1\n",
    "print(count)\n",
    "\n",
    "#  Sequence length : Length of each training example\n",
    "#  Sequence length is varying from 1 to 74, we have to choose a dimension and accordingly all training exapmles would be\n",
    "#  padded or truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X_new = keras.preprocessing.sequence.pad_sequences(sequences=X, maxlen=64, dtype='float32', padding='post', truncating='post', value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 64, 150)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = tf.keras.utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "[1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(Y))\n",
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "###-----------------------------CNN Model---------------------------###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(seq_length, embedding_size, n_y):\n",
    "    \n",
    "#     Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "#     Arguments:\n",
    "#     n_H0 -- scalar, height of an input image\n",
    "#     n_W0 -- scalar, width of an input image\n",
    "#     n_C0 -- scalar, number of channels of the input\n",
    "#     n_y -- scalar, number of classes\n",
    "        \n",
    "#     Returns:\n",
    "#     X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype \"float\"\n",
    "#     Y -- placeholder for the input labels, of shape [None, n_y] and dtype \"float\"\n",
    "    \n",
    "\n",
    "    ### START CODE HERE ### (≈2 lines)\n",
    "    X = tf.placeholder(dtype = tf.float32, shape=(None,seq_length,embedding_size,1))\n",
    "    Y = tf.placeholder(dtype = tf.float32, shape=(None,n_y))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def initialize_parameters(filter_size,embedding_size,num_filters):\n",
    "#     # Initializes weight parameters\n",
    "#     W = tf.get_variable(\"W\",[filter_size,embedding_size,1,num_filters],dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer(seed=0),regularizer = tf.contrib.layers.l2_regularizer(scale=0.1))\n",
    "#     return W\n",
    "\n",
    "def initialize_parameters(filter_sizes,embedding_size,num_filters):\n",
    "    # Initializes weight parameters\n",
    "    W1 = tf.get_variable(\"W1\",[filter_sizes[0],embedding_size,1,num_filters],dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer(seed=0),regularizer = tf.contrib.layers.l2_regularizer(scale=0.1))\n",
    "    W2 = tf.get_variable(\"W2\",[filter_sizes[1],embedding_size,1,num_filters],dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer(seed=0),regularizer = tf.contrib.layers.l2_regularizer(scale=0.1))\n",
    "    W3 = tf.get_variable(\"W3\",[filter_sizes[2],embedding_size,1,num_filters],dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer(seed=0),regularizer = tf.contrib.layers.l2_regularizer(scale=0.1))\n",
    "    W4 = tf.get_variable(\"W4\",[filter_sizes[3],embedding_size,1,num_filters],dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer(seed=0),regularizer = tf.contrib.layers.l2_regularizer(scale=0.1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"W2\": W2,\n",
    "                  \"W3\": W3,\n",
    "                  \"W4\": W4}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def forward_propagation(X,filter_sizes,embedding_size,num_filters,seq_length):\n",
    "#     P2 = []\n",
    "#     for filter_size in filter_sizes:\n",
    "#         W = initialize_parameters(filter_size,embedding_size,num_filters)\n",
    "#         Z = tf.nn.conv2d(X,W,strides=[1,1,1,1],padding=\"SAME\")\n",
    "#         A = tf.nn.relu(Z)\n",
    "#         P = tf.nn.max_pool(A,ksize=[1,seq_length-filter_size+1,1,1],strides=[1,1,1,1],padding=\"SAME\")\n",
    "#         P2.append(P)\n",
    "#     Z2 = tf.contrib.layers.fully_connected(P2,41,activation_fn = None)\n",
    "#     return Z2\n",
    "\n",
    "def forward_propagation(X,filter_sizes,embedding_size,num_filters,seq_length,parameters):\n",
    "    P = []\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    W3 = parameters['W3']\n",
    "    W4 = parameters['W4']\n",
    "    #W1 = initialize_parameters(filter_sizes[0],embedding_size,num_filters)\n",
    "    Z1 = tf.nn.conv2d(X,W1,strides=[1,1,1,1],padding=\"SAME\")\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    P1 = tf.nn.max_pool(A1,ksize=[1,seq_length-filter_sizes[0]+1,1,1],strides=[1,1,1,1],padding=\"SAME\")\n",
    "    P.append(P1)\n",
    "    \n",
    "    #W2 = initialize_parameters(filter_sizes[1],embedding_size,num_filters)\n",
    "    Z2 = tf.nn.conv2d(X,W2,strides=[1,1,1,1],padding=\"SAME\")\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    P2 = tf.nn.max_pool(A2,ksize=[1,seq_length-filter_sizes[1]+1,1,1],strides=[1,1,1,1],padding=\"SAME\")\n",
    "    P.append(P2)\n",
    "    \n",
    "    #W3 = initialize_parameters(filter_sizes[2],embedding_size,num_filters)\n",
    "    Z3 = tf.nn.conv2d(X,W3,strides=[1,1,1,1],padding=\"SAME\")\n",
    "    A3 = tf.nn.relu(Z3)\n",
    "    P3 = tf.nn.max_pool(A3,ksize=[1,seq_length-filter_sizes[2]+1,1,1],strides=[1,1,1,1],padding=\"SAME\")\n",
    "    P.append(P3)\n",
    "    \n",
    "    #W4 = initialize_parameters(filter_sizes[3],embedding_size,num_filters)\n",
    "    Z4 = tf.nn.conv2d(X,W4,strides=[1,1,1,1],padding=\"SAME\")\n",
    "    A4 = tf.nn.relu(Z4)\n",
    "    P4 = tf.nn.max_pool(A4,ksize=[1,seq_length-filter_sizes[3]+1,1,1],strides=[1,1,1,1],padding=\"SAME\")\n",
    "    P.append(P4)\n",
    "    \n",
    "    Z5 = tf.contrib.layers.fully_connected(P4,41,activation_fn = None)\n",
    "    return Z5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z5, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z5 -- output of forward propagation (output of the last LINEAR unit), of shape (number of examples,41)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z5\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z5, labels = Y))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 8, seed = 0):\n",
    "    np.random.seed(seed)            \n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((2,m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        \n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:,num_complete_minibatches*mini_batch_size:m]\n",
    "        mini_batch_Y = shuffled_Y[:,num_complete_minibatches*mini_batch_size:m]\n",
    "        \n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.007,\n",
    "          num_epochs = 10, minibatch_size = 8, print_cost = True):\n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep results consistent (tensorflow seed)\n",
    "    seed = 3                                          # to keep results consistent (numpy seed)\n",
    "    \n",
    "    # To be used if not using stochastic\n",
    "    (m, seq_length, embedding_size,nc) = X_train.shape             \n",
    "    ##-----------------------------------------###\n",
    "    \n",
    "    \n",
    "    ## To be used if using Stochastic ##\n",
    "#     m = X_train.shape[0]\n",
    "#     seq_length = X_train.shape[2]\n",
    "#     embedding_size = X_train.shape[3]\n",
    "#     nc = X_train.shape[4]\n",
    "    ##------------------------------------####\n",
    "    \n",
    "    \n",
    "    \n",
    "    n_y = Y_train.shape[1]            # 2 - stochastic;  1 - otherwise                            \n",
    "    costs = []                                        # To keep track of the cost\n",
    "    filter_sizes = [2,3,5,7]\n",
    "    num_filters = 5\n",
    "    # Create Placeholders of the correct shape\n",
    "    X, Y = create_placeholders(seq_length, embedding_size, n_y)\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(filter_sizes,embedding_size,num_filters)\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "\n",
    "    Z5 = forward_propagation(X,filter_sizes,embedding_size,num_filters,seq_length,parameters)\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(Z5, Y)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initialize all the variables globally\n",
    "    init = tf.global_variables_initializer()\n",
    "     \n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            _, temp_cost = sess.run([optimizer, cost], feed_dict = {X:X_train, Y:Y_train})  #Batch Gradient Descent\n",
    "\n",
    "            minibatch_cost = 0.\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\n",
    "                ### START CODE HERE ### (1 line)\n",
    "                _ , temp_cost = sess.run([optimizer, cost], feed_dict = {X: minibatch_X, Y: minibatch_Y})     # mini_batch gradieent descent\n",
    "                ### END CODE HERE ###\n",
    "                \n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "#             stochastic_cost=0    \n",
    "#             for i in range(0,m):\n",
    "#                 _, temp_cost = sess.run([optimizer, cost], feed_dict = {X:X_train[i], Y:Y_train[i]}) \n",
    "#                 stochastic_cost += temp_cost/m\n",
    "#                 if(i%10==0):\n",
    "#                     print(\"Cost after\",i,\"iterations =\",stochastic_cost)\n",
    "                \n",
    "            #Print the cost every epoch\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(minibatch_cost)\n",
    "#             if print_cost == True and epoch % 5 == 0:\n",
    "#                 print (\"Cost after epoch %i: %f\" % (epoch, stochastic_cost))\n",
    "#             if print_cost == True and epoch % 1 == 0:\n",
    "#                 costs.append(stochastic_cost)\n",
    "#             if print_cost == True and epoch % 5 == 0:\n",
    "#                 print (\"Cost after epoch %i: %f\" % (epoch, temp_cost))\n",
    "#             if print_cost == True and epoch % 1 == 0:\n",
    "#                 costs.append(temp_cost)\n",
    "        \n",
    "        \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "        # Calculate the correct predictions\n",
    "        predict_op = tf.argmax(Z5, 1)\n",
    "        correct_prediction = tf.equal(predict_op, tf.argmax(Y))\n",
    "        \n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print(accuracy)\n",
    "        #accuracy = tf.Print(accuracy, [accuracy], message=\"Accuracy: \")\n",
    "        #print(sess.run(accuracy))\n",
    "        train_accuracy = accuracy.eval({X: np.squeeze(X_train,axis=1), Y: np.squeeze(Y_train,axis=1)})\n",
    "        test_accuracy = accuracy.eval({X: np.squeeze(X_test,axis=1), Y: np.squeeze(Y_test,axis=1)})\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Test Accuracy:\", test_accuracy)\n",
    "        \n",
    "        #precision = tf.metrics.precision(Y_test,correct_prediction)\n",
    "        #recall = tf.metrics.recall(Y_test,correct_prediction)\n",
    "        \n",
    "        #print(\"Precision =\",precision)\n",
    "        #print(\"Recall=\",recall)\n",
    "        \n",
    "        #F1_score_sklearn = f1_score()\n",
    "        #F1_score_tf = tf.contrib.metrics.f1_score(Y_test,predictions) \n",
    "        #print(\"F1_score=\",F1_score_tf)\n",
    "                \n",
    "        #return train_accuracy, test_accuracy, predict_op,parameters\n",
    "        return predict_op,correct_prediction,parameters,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 64, 150)\n",
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X))\n",
    "print(np.shape(Y))\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(670, 64, 150)\n",
      "(670, 2)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.shape(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.expand_dims(X_train,axis=3)\n",
    "X_test = np.expand_dims(X_test,axis=3)\n",
    "#Y_train = np.expand_dims(Y_train,axis=1)\n",
    "#Y_test = np.expand_dims(Y_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(670, 64, 150, 1)\n",
      "(670, 2)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.shape(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-39-fe3af9b9e5f9>:14: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_op,correct_prediction,parameters,accuracy = model(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
